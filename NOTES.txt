kubectl get all -l foo=bar - to get all of items with labels
kubectl replace --force -f sample.yaml - replace the created item with a new one(delete+apply/create)

1. ##################### To taint a Node: #####################
    kubectl taint nodes <node-name> key=value:(taint-effect) 
    taint-effect:
        1. NoSchedule
        2. PreferNoSchedule
        3. NoExecute
2. ##################### To make pod tolerant: #####################
    spec:
    tolerations:
    - key: "app"
        operator: "Equal"
        value: "blue"
        effect: NoSchedule

3. ##################### Node Labels and Affinity: #####################
    kubectl label nodes <node-name> <label-key>=<label-value> - put a label on a node
    To make pod select a node:
    spec:
    nodeSelector:
        label-key: label-value
4. ##################### Pod Affinity: #####################
    spec:
    affinity:
        nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:    --->>> will NOT deploy if no label found
                            OR
        preferredDuringSchedulingIgnoredDuringExecution:   --->>> will deploy if no label found
            nodeSelectorTerms:
            - matchExpressions:
            - key: label-key
                operator: In --->>> https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators 
                values:
                - label-value1
                - label-value2
                - ...

5. ##################### Pod RESOURCE and LIMIT: #####################
    resources:
      requests:     --->>> How much will it want on start
        memory: 1Gi
        cpu: 1
      limits:       --->>> How much will it allow to get
        memory: 2Gi -->> If memory is exeded it will terminate the pod with OOM status
        cpu: 3      -->> CPU will be throteled

Requests SET and NO limits are the most optimal way
We can define a LimitRange - it can be set for NAMESPACEs:
LimitRange only apply to newly created pods!!

    apiVersion: v1
    kind: LimitRange
    metadata:
      name: cpu-resource-constraint
    spec:
      limits:
      - default:
          cpu: 500m
        defaultRequest:
          cpu: 500m
        max:
          cpu: 1
        min:
          cpu: 100m
        type: Container 

Another way is to set ResourceQuota - it's a Namespace object:
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: some-rq 
    spec:
      hard:
        requset.cpu: 4
        request.memory: 4Gi
        limits.cpu: 10
        limits.memory: 10Gi

PODs cannot be fully edited, most of fields are blocked (image, dedlineSeconds and tolerations can)
    kubectl get pod <pod-name> -o yaml > the-pod-to-edit.yaml --->>> and then kubectl delete <pod-name> --->>> kubectl create -f the-pod-to-edit.yaml

Deployment template can be change all together 
So it's better to edit a deploymet pod in a deployment file
    kubectl edit deploy <deployment-name> 

*** In describe there is LastState: <Reason> - for pod termination *** 

6. ##################### DaemonSets - simmilar to ReplicaSet, it's used for monitoring and networking #####################
                          *** Easy to modify deployed yaml for DaemonSet yaml ***
Past 1.12 it uses NodeAffinity to deploy to nodes
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: monitoring-d
    spec:
      selector:
        matchLabels:
          app: monitoring
      template:
        metadata:
          labels:
            app: monitoring
        spec:
          containers:
          - name: monitoring-agent
            image: minitoring-agent

7. ##################### Static PODs - created on Node by "kubelet" in a designated folder #####################
*** They do not have the random number in the name???!!! ***
*** LERN WHERE THE SERVICES FILES RESIDE ***      --->> By default in /etc/kubernetes/manifests 
It's defined in the kublet.service file as either:
 - --pod-manifest-path=/etc/some/path
 - --config=kubeconfig.yaml => staticPodPath=/etc/some/path


8. ##################### Multiple Schedulers: #####################
Must have different names 
Deploy additional scheduler as a POD:           --->>> To assign a POD to scheduler need to add "schedulerName: into spec: 

    **my-schedul.yaml**
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: my-scheduler               --->>> the uniqe name
    leaderElection:                             --->>> Only one leader can be selected
      leaderElect: true
      ressourceNamespace: kube-system
      ressourceName: lock-object-my-scheduler

    **custom-scheduler-as-pod.yaml**
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-custom-scheduler
      namepsace: kube-system
    spec:
      containers:
      - command:
        - kube-scheduler
        - --address=127.0.0.1
        - --kubeconfig=/path/to/scheduler.conf  --->> This drive has to be mount first into the pod
        - --config=/path/to/my-schedul.yaml     --->> This drive has to be mount first into the pod

        image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
        name: kube-scheduler

 9. ##################### Scheduler Profiles ########################
 Pods first end up in scheduling queue          --->>> this can be modified assigining priorityClass: on pod spec: | this class has to be created via "kind: PriorityClass"
Then it scores the nodes, the more resource stay the better
Then it's binding
Phases:
Scheduling Queue    ->     Filtering      ->       Scoring      ->      Binding      -->> Phases
  (queueSort)              (filter)               (score)               (bind)      -->> Extension Points - these are the extensions to which a custom plugin can bind(there are pre and post)
- PrioritySort      - NodeRessourceFit      - NodeRessourceFit     - DefaultBinder  -----------------------------
                    - NodeName              - ImageLocality                         -->> These are the pluggins
                    - NodeUnscheduable

10. ################# Monitoring ########################
For Minikube - minikube addons enable metrics-server 
Others - git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git 
** kubectl top node **  -->> to monitor nodes
** kubectl top pod **   -->> to monitor pods 

11. ##################### Application Lifecycle #####################
This applies to deployments
kubectl rollout status deployment/my-deployment         --->>> Checking the status of rollout on deployment 
kubectl rollout history deployment/my-deployment        --->>> Checking the Revisions history 

kubectl apply -f my-deployment.yaml                     --->>> Rollout a change if there are changes made to file
kubectl rollout undo deployment/my-deployment           --->>> Back out of a rollout

Deployment strategies:
  1. Recreate - All pods go down and get replaced
  2. Rolling Update(default) - Pods go down one by one and replace the same way

12. ##################### Application Configuration: #####################

  containers: 
  - name: ...
    command:                          --->>> RUN COMMAND IN A CONTAINER
    - "command"     -->> ENTRYPOINT
    args:           -->> CMD

kubectl run nginx --image=nginx --command -- <CMD> <ARG> ... <ARG>      -->> This -- sends command into container, otehr words separates commands for pod and container

POD ENVIRONMENT:
    spec:
      containers:
        env:        --->>> It's an Array
        - name: APP_VERSION
          value: "1.0.1"
        - name: APP_ENV
          value: "prod"

#####################  ConfigMaps: ######################  
These are sets of ENVIRONMENT variables for pods

** ConfigMap: **                                    --->>> To crate ConfigMap: kubectl create configmap <name> --from-literal=APP_ENV=prod 
    apiVersion: v1                                                         OR: kubectl create -f myconfig-map.yaml 
    kind: ConfigMap
    metadata:
      name: my-config-map
    data:                                           --->>> Uses data instead of spec
      APP_VERSION: 1.0.1
      APP_ENV: prod

Use configmap in Pod:
  spec:
    containers:
    - ...
      envFrom:
        - configMapRef:
          name: my-config-map
        - ...

#####################  Secrets: ##################### 

Work tha same as ConfigMaps can be created with     --->>> kubectl create secret <name> --from-literal=KEY=value
Best to use "echo -n 'value' | base64" ==> then store it in plain text
This is still bad and can be doceded - use Encription At Rest
Additionaly secrets and configmaps can be injected into POD via:

  spec:                            --->>> whole secret 
    containers:
    - ...
      envFrom:
        - secretRef:
          name: secret
        - ...
  spec:                             --->>> single secret 
    containers:
    - env:
        name: secrete_passwrd
        valueFrom:
          secretKeyRef:
            name: <name_of_secret_object>
            key: <the_key_of_desired_secret>

  containers:                       --->>> volume mount
  - name: ...
    volumeMounts:
    - name: the-secret-volume
      mountPath: "/path/to/secret"

  volumes:                          --->>> location to use for volume
  - name: the-secret-volume
    secret:
      secretName: db-passwrd

14. ##################### Multi Container Pod: #####################

Share the same network and can call each other localhost
Part of the same Pod object, containers: definition is an array hence:

  spec:
    containers:
    - name: the-big-db
      image: mongodb
      ...
    - name: the-small-monitor
      image: alpine
      ...

There are 3 common patterns for multicontainer POD:
-SIDECAR
-ADAPTER
-AMBASADOR

##################### Init Containers #####################
They run before the actuall containers, one by one untill completion
If one fails the pod is restarted untill success or if restartPolicy: Never it fails

You can get logs from that containers as well
** kubectl logs <pod-name> -c <init-container-name>

Might be used as a healthcheck for another service:
for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1

Or to clone a repo to a mounted volume

##################### Cluster Maintanance #####################

** NODE Draining and cordining: **
kubectl drain node01              --->>> This will shut down all pods on nodes and mark the node as unavailable for use in upgrading 
                                          Pods have to be part of Deployment/replicaSet for this to work
kubectl cordon node01             --->>> This just MARKS the node as unavailable
kubectl uncordon node01           --->>> This just UN-MARKS the node so that pods can be scheduled back on it

** Kubernetes versions **

https://github.com/kubernetes/kubernetes

##################### CLUSTER Update #####################
https://v1-27.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

Kube-apiserver is the brain X
ControllManager and kube-scheduler might be 1 version lower X-1
kubelet and kube-proxy might be 2 versions lower X-2
kubectl can be 1 ver higher or lower X-1 or X+1 

* Upgrade Options: *
1. CloudProvider - will do it for you when installed on CloudProvider
2. kubeadm - when you installed with kubeadm
3. The Hard Way - when you installed all componets by yourself

* Upgrade schema: *
ALWAYS UPGRADE ONE MINOR UP 1.10.1 -->> 1.11.0 or 1.11.0 -->> 1.12.0
1st - Upgrade Master Nodes
2nd - Upgrade the worker Nodes

* Upgrade strategy: *
Upgrade Master then all nodes at once - tha 
Upgrade Master then node by node - less app downtime
Upgrade MAster then provision new machines with new version - Cloud Provider

* How to do it with kubeadm *

  *** master node upgrade ***
  kubeadm upgrade plan
  apt-get upgrade kubeadm=1.12.0-00
  kubeadm upgrade apply v1.12.0         -->> This command will be displayd by the "plan" command
  --- kubeapi is upgraded ---

  *** kubelet upgrade ***               -->> There is no kubelet on master node when installing from scrach
  apt-get upgrade kubelet=1.12.0
  systemctl restart kubelet
  --- kubelet is upgraded ---

  *** Upgrade the nodes ***
  kubectl drain <node-name>
  ssh into node
  apt-get upgrade kubeadm=1.12.0-00
  apt-get upgrade kubelet=1.12.0
  kubeadm upgrade node config --kubelet-version v1.12 
  systemctl restart kubelet
  ssh back
  kubectl uncordon <node-name>
  --- Upgrade of nodes done --- 

##################### CLUSTER BackUp #####################

The configuration in declarative way is saved in files
But you can ask API for all imperative changes:
  kubectl get all -A -o yaml > current-config.yaml

*VELERO* - 3rd party

ETCD (export ETCDCTL_API=3 --> before use)- holds all the info of cluster so the location of it's data can be backed up
  ps -aux | grep etcd
  systemctl status etcd.service

OR
########### Create Backup ##############
Tell the etcdctl to snapshot the config:

  etcdctl snapshot save saved-config.db 

Side Note: 
 - you might need to use flags:
    --cacert
    --cert 
    --endpoints=[127.0.0.1:2379]
    --key
This above is used when etcdctl is comunicating with a POD, this is the case with kubeadm deployment
You need to specify it for the ETCDCTL to know from where to get the backup
If the ETCD is TLS encrypted

########### Restore Backup ###########

To use it for restore you have to stop kube-apiserver.service:
  systemctl stop kube-apiserver
  etcdctl snapshot restore saved-config.db --data-dir /var/lib/new-data-dir

And then configure etcd.service or POD to use new data directory:
  --data-dir=/var/lib/new-data-dir

For POD you have to fiddle with pod definition in manifest folder:
  etcdctl --data-dir=/var/lib/new-data-dir

And add the new directory as a volume path:
  volumes:
  - hostPath:
      Path: /var/lib/new-data-dir

  Reload services and start back! 

kubectl config - managing multiple clusters, changing context
etcdctl \
 --endpoints=https://127.0.0.1:2379 \
 --cacert=/etc/etcd/pki/ca.pem \
 --cert=/etc/etcd/pki/etcd.pem \
 --key=/etc/etcd/pki/etcd-key.pem \
  member list

kube-apiserver - has the etcd address           -->> POD
kubelet - might also have this configuration    -->> SERVICE

If ETCD is a remote cluster - the backups have to be made with --endpoints --cacert --cert --key 

Important if ETCD is a service and you create a --data-dir=<new-dir> you HAVE to allow only etcd user to access it:
chown -R etcd /<new-dir> 

###################### Security ######################

Communication betwen API and other control services uses TSL

## USERS ## 
Auth via kube-apiserver:
  Basic option is to create a CSV file with password,user,uid columns
  and pass it as --basic-auth-file=<the-file.csv> in kube-apiserver definition

  Then you need to create according roles for them and they can authenticate on API server:
  curl -v -k https://localhost:6443/api/v1/pods -u user:password

## Certificates ##

  "The Hard Way"
You have to create all the certs by yourself - this is tedious

  "kubeadm" - auto provisioning tool
This creates the certs for you and the information about them(location) is stored in the kube-apiserver.yml definition

To check if the cert is  good run 
"openssl x509 -in /path/to/certificate.crt -text -noout"
Then you need to check if the cert is Expired(date)                     --->>> you can check expiration with "kubeadm certs check-expiration"
If the Issuer is valid
Subject
Organization
And alternative names

The list of certificates is available on - https://kubernetes.io/ 

To check for issues with certs go:
  journalctl -u etcd.service -l   - the hard way
  kubectl logs etcd-pod           - the kubeadm way
  docker logs k8s-container-etcd  - if the kubectl broke down
  crictl ps -a                    - will also show the logs of selected pod if kubectl is broken
  docker ps -a                    - will do the same

## Controlplane ports ##

  TCP	Inbound	6443	Kubernetes API server	All
  TCP	Inbound	2379-2380	etcd server client API	kube-apiserver, etcd
  TCP	Inbound	10250	Kubelet API	Self, Control plane
  TCP	Inbound	10259	kube-scheduler	Self
  TCP	Inbound	10257	kube-controller-manager	Self

## Certificate API and Requests ##
DETAILS - https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/ 

Control Manager is responsible for managing the cert rotation:
- CSR-APPROVING
- CSR-SIGNING 
The Cert and Key for that is specified in kube-controll-manager.yaml in /etc/kubernetes/manifest/ folder

Certificate Signing Requests commands:

kubectl get csr
kubectl certificate approve/deny <name of certificate>

    apiVersion: certificates.k8s.io/v1
    kind: CertificateSigningRequest
    metadata:
      name: <name of the object>
    spec:
      groups:
      - system:authenticated
      - ...
      request: <base64 encoded CSR>
      signerName: kubernetes.io/kube-apiserver-client
      usages:
      - client auth 
      - ...

## KubeConfig ##

    KubeConfig is a file where all the certificates for kubectl are stored.
  Or rather authentication details for cluster and user, can be multible of both

Default is to look for file config in: 
  $HOME/.kube/config

To see the current control file use:
  kubectl config view 

To change current context use:
  kubectl config use-context <name of context>

To use another file without replacing the original use:
  kubectl config --kubeconfig my-kube-config [COMMAND]

Kubeconfig.yaml
  apiVersion: v1
  kind: Config
  current-context: some-kube-admin@some-kube-server                               --->>> Default context to use
  clusters:
   - name: some-kube-server
     cluster: 
       certificate-authority: /etc/kubernetes/pki/ca/ca.crt                        --->>> Best use full paths
       server: https://some-kube-cluster:6443
   - ...
  contexts:
   - name: some-kube-admin@some-kube-server
     context:
       cluster: some-kube-server
       user: some-kube-admin
       namepsace: some-namespace 
   - ...
  users:
   - name: some-kube-admin
     user:
       client-certificate: etc/kubernetes/pki/user/client-cert.crt
       client-key: etc/kubernetes/pki/user/client-key.key
   - ... 


### API Groups ### 

  All resources in K8s are a part of API group you can access it via:
    curl https://kube-api-server:6443 -k --cert <cert file> --key <key-file> --cacert <CA cert file>

###################### Authorization ######################  

  We have few methods of authorization into cluser:
    - Node - that's access within the cluser, for users and nodes 
    - A(tribute)BAC - it's a file in which you assing access in JSON format for users and groups 
    - R(ole)BAC - you create roles to which you grant access these roles are then assigned for users and groups
    - Webhook - it's a 3rd party provider to which API will call for verification of access
  and also:
    - AlwaysAllow
    - AlwaysDeny 
  
  Mode is speified in kube-apiserver configuration with(set to AlwaysAllow as default):
    --authorization-mode=RBAC,Webhook ...

## Role Based Access Control ## 

To create role/rolebinding 
  kubectl create role/rolebinding

To edit role/rolebinding
  kubectl edit role/rolebinding

To get roles on cluster
  kubectl get roles

To get Role bindings
  kubectl get rolebindings

Describe the role/binding
  kubectl describe role/rolebinding <name>

To check for access/permision you can 
  kubectl auth can-i create deploymens
  kubectl auth can-i delete nodes 

For others
  kubectl auth can-i create deploymens --as some-user
  kubectl auth can-i delete nodes --as some-other-user --namespace default

Role example:
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      namespace: default
      name: pod-reader
    rules:
    - apiGroups: [""]                                                          -->> "" indicates the core API group("v1")
      resources: ["pods"]                                                      -->> Resource to which the access is granted
      verbs: ["get", "watch", "list"]                                          -->> actions that can be taken
      resourceNames: ["finance-pods", "hr-pods"]
    - apiGroups: [""]
      resource: ["ConfigMap"]
      verbs: ["list", "create"]
  
Role binding example(This role binding allows "jane" to read pods in the "default" namespace):
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: read-pods
      namespace: default
    subjects:
    - kind: User
      name: jane                                            -->> "name" is case sensitive
      apiGroup: rbac.authorization.k8s.io
    - ...
    roleRef:
      kind: Role
      name: pod-reader                                      -->> You need to already have a Role named "pod-reader" in that namespace
      apiGroup: rbac.authorization.k8s.io

## Cluster Roles ##

Used to assign access to cluster wide resources(not namespaced) - Storage, Nodes as well as namespaced resources
You can assign a role to list pods in all namespaces 


kubectl api-resources --namespaced=true                   -->> Get the namespaced resources(pods, deployments, jobs, replicas)
kubectl api-resources --namespaced=false                  -->> Get the rest of resources(nodes, PVs, clusterRoles, clusterRolebindings)

Cluster Role example:
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:                                             -->> "namespace" omitted since ClusterRoles are not namespaced
      name: secret-reader
    rules:
    - apiGroups: [""]
      resources: ["secrets"]
      verbs: ["get", "watch", "list"]
    - apiGroups: [""]
      resources: ["nodes"]
      verbs: ["list", "get", "create"]

Cluster Role Binding example:
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: read-secrets-global
    subjects:
    - kind: Group
      name: manager
      apiGroup: rbac.authorization.k8s.io
    - kind: User
      name: jhon
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: ClusterRole
      name: secret-reader
      apiGroup: rbac.authorization.k8s.io

#### Service Accounts ####

Bots that can do work for you like for cluster scaling or other Maintanance actions
They are created with token already - stored as a secret

!!! Past 1.24 the token is not created automatically !!!
!!! and it is not a secrete object !!!

But the token gets a finite ttl as opose to pre 1.22

To create a service account
  kubectl create serviceaccount <name>

To create a token for account
  kubectl create token <name of serviceaccount>

They can be used with RoleBinding 
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: read-pods
      namespace: default
    subjects:
    - kind: ServiceAccount
      name: dashboard-sa # Name is case sensitive
      namespace: default

#### Private Image Repos #### 

Like with docker but to allow K8s to authenticate on the server you need to create secret.
K8s has a special secret type docker-registry:
  kubectl create secret docker-registry reg-cred \
    --docker-server=  \
    --docker-username=  \
    --docker-password=  \
    --docker-email=   

To use it in deployment/pod
    apiVersion: v1
    kind: Pod
    metadata:
      name: private-reg
    spec:
      containers:
      - name: private-reg-container
        image: <your-private-image>
      imagePullSecrets:                               --->>> Specifing the credentials
      - name: regcred

#### Security context ####

Docker and K8s allow to run processes as different users
You can specify the user with wich the container is run in both

Example for POD:
    ...
    spec:
      securityContext:
        runAsUser: 1000
      containers:
        ...

Example for container within POD:
    ...
    spec:
      containers:
      - image: neginx:alpine
        name: some-web-container
        securityContext:
          runAsUser: 1000                             --->>> runAsUser only accepts int64 so no "root" use "id" command to get user id
          capabilities: 
            add: ["MAC_ADMIN"]                        --->>> Only available for container level

###################### Network Policies ######################

Ingrees and Egrees - allow or deny trafic in the cluster

Network Policy Example: 
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: test-network-policy
      namespace: default
    spec:
      podSelector:                                         --->>> Match object for which the np will apply - POD of name "db"
        matchLabels:
          role: db
      policyTypes:                                         --->>> If only Ingress is specified here only ingress will be modified
        - Ingress                                                 so egress would not be affected and the trafic out will flow at will
        - Egress
      ingress:
      - from:
        - ipBlock:                                          --->>> This is for adrresses outisde the cluster - eg. Backup Server
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
      1.========================                            --->>> This will allow PODS from namespace myproject and pods of name frontend to access
        - namespaceSelector:                                
            matchLabels:
              project: myproject
        - podSelector:                                      
            matchLabels:                                        
              role: frontend
      2.========================                            --->>> If this is done like this it's and AND operator
        - namespaceSelector:                                      PODS from namespace myproject of name frontend can access   
            matchLabels:
              project: myproject
          podSelector:                                      
            matchLabels:                                           
              role: frontend
        ========================
        ports:                                              --->>> On what port the communication will be allowed - either outside or inside cluster
        - protocol: TCP
          port: 6379
      - from:                                               --->>> You can add more of FROM and TO objects
        - podSelector:
            matchLabels:
              name: backend
          ports:
          - protocol: TCP
            port: 8080
      egress:                                               --->>> All of the above apply to egress as well 
      - to:
        - ipBlock:
            cidr: 10.0.0.0/24
        ports:
        - protocol: TCP
          port: 5978

###################### Storage ######################

Kubernetes uses CSI(Container Storage Interface) - hehe - for storage driver

#### Volumes & Mounts ####

Volumes can be mounted the same way as in docker compose - just with more description

As bind:
    apiVersion: v1
    kind: Pod
    metadata: 
      name: some-pod
    spec:
      containers:
      - name: some-container
        image: alpine
        command: ["/bin/sh", "-c"]
        args: ["echo 123 >> /opt/data.txt"]
        volumeMounts: 
        - mountPath: /opt
          name: data-volume

    volumes:                                        
    - name: data-volume                                     
      hostPath:                                     --->>> This is local(hostPath) version the block can be replaced 
        path: /home/user/data                               by virutualy any storage provider - AWS, Cephfs etc. 
        type: directory
    ========================== 
    volumes:                                         --->>> AWS as a example
    - name: data-volume
      awsElasticBlockStore:                          --->>> (not available in v1.27) - now the go to is sci 
        volumeID: <the ID>
        type: xfs 

#### Persistent Volumes and Claims ####

We have: 

Persistent Volume - a chunk of space from which PODs can reserv a part with claim
Persistent Volume Claim - a request from POD to get the space it's 1:1 relation.
                          It's rather common to create the Claim that will call upon Storage Class to create the PV as needed

PVC doesn't need to be cteated with a PV in mind - the cluster will try and automatically select a PV for a Claim 
The PVC wil get the whole space of PV that it was bound to - even if the request asked for less 

Persistent Volume example:
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: example-pv
    spec:
      capacity:
        storage: 100Gi
      awsElasticBlockStore:                          --->>> (not available in v1.27) - now the go to is sci 
        volumeID: <the ID>
        type: xfs       

Persistent Volume Claim example:
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: my-pvc
    spec: 
      accessModes:
      - ReadWriteOnce
      resources:
        request:
          storage: 500Mi
      persistentVolumeReclaimPolicy: Retain/Delete/Recycle      --->>> Select what will happen to the volume
                                                                        - Retain - will remain until deleted(not to reuse)
                                                                        - Deleted - when claim goes away the volume goes away
                                                                        - Recycle - the volume will be available for other calaims

Usage in POD: 
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-pod
    spec:
      containers:
        - name: my-frontend
          image: nginx
          volumeMounts:
          - mountPath: "/var/www/html"
            name: my-pvc
      volumes:
        - name: my-pvc
          persistentVolumeClaim:
            claimName: myclaim

#### Storage Class ####

Storage Class is like a PV creation definition.
It uses the Cloud Provider od local disk to provision a PV on a specified location
In PVC you then specify what StorageClass to use and the space is provisioned

StorageClass example:
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: aws-gc2-storageclass
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp2
    reclaimPolicy: Retain
    allowVolumeExpansion: true
    volumeBindingMode: Immediate

###################### Networking ######################

CNI takes care of networking within cluster -- DOCKER does not adheer to CNI it uses CNM and so K8s has a method to go around it 
It creates containers with "none" network and then uses CNI plugins to add it to required network 

Set of command to keep in mind when working with network on Linux:
  ip link
  ip addr
  ip addr add <CIDR> dev <device-to-which-add>
  ip addr show bridge                                 --->>> This is usually CNI configured port
  ip route
  ip route add <CIDR-destination> via <IP-gateway>
  cat /proc/sys/net/ipv4/ip_forward                   --->>> file, content is 0 - false, 1 - true, for IP forwarding
  arp
  netstat -tulnp                                      -->> All ports
  netstat -plnt                                       -->> Ports that listen for connection

K8s supports a variaety of networking addons which names can be foud in docu
https://kubernetes.io/docs/concepts/cluster-administration/addons/ 

On exam all CNI tools are provided
CNI is always there as K8s does not provide a native solution

K8s does not solve the problem of networking within cluster - meaning:
K8s sets rules that:
  - Every POD should have an IP address
  - Every POD should be able to communicate with another POD with that address
  - Either on the same node or within the cluster

CNI is configured in the kubelet.service on each node under:
  --network-plugin=cni
  --cni-bin-dir=/opt/cni/bin                          --->>> Those 2 are default? 
  --cni-conf-dir=/etc/cni/net.d

CNIs are usually a POD running in specific namespace and configuration to them is mounted as volumes

#### Services and Networking ####

Services are not node specific - they exist on the cluster and hence do not exist at all...
kube-proxy is taking care of putting correct entry in iptables to route the request to service to correct pod
  10.10.10.2(a NodePort service address) --> 10.20.0.12:8080(a POD that will hosts a web service on port 8080)
    NodePort will chose a node port to expose from range 30050-30257 or sth

These rules can be found on host when running:
  iptables -L -t nat 

And more can be seen in 
  /var/log/kube-proxy.log

Service IP range is configured in the kube-apiserver.yaml manifest:
    - --service-cluster-ip-range=10.96.0.0/12

To check kube-proxy method check the kube-proxy logs:
  kubectl logs -n kube-proxy-{id} 

#### Cluster DNS ####

By default cluster has a DNS installed
And PODs can comunicate with services by their name
PODs do not get a DNS record by default but this can be overwritten 

SERVICES:
For same namespce:
  ping webfrond 

For other namespace:
  ping webfront.<namespace-name> = ping webfront.frontend

DNS further clasifies this as:
  ping webfront(service).frontend(namespace).svc(all-services).cluster.local(all pods and services)
  ping webfront.frontend.svc.cluster.local

PODS:
For same namespce:
  ping ip-of-the-pod
  ping 10-244-0.15

For other namespace:
  ping 10-244-0.15.webfront

Full:
  ping 10-244-0.15.frontend.pod.cluster.local

#### INGRESS ####

It does not come as default! - you have to deploy it, the Ingress Controler

This is an API object - like PODs, Deployments, Services, ClusterRoles etc.
It manages EXTERNAL access to the services inside cluster - typically HTTP
This my includes:
  - load balancing
  - SSL termination
  - name-based virtual hosting

NOTE: Ingress is frozen - no new features are added 
      New stuff is "Gateway API" 

Ingress is deployed as 
  1. Ingress Controler - the solution that provides the load-balancing, reverse proxy, cert assignment(Nginx, HAProxy, Traefik, Istio)
  2. Ingress Resource - the set of configurations that are passed to controller telling it how to behave

Ingress Controler Deployment example:                             -->> This is usually available on provider site (Nginx, Traefik, HAProxy etc.)
    apiVersion: extensions/v1beta1 
    kind: Deployment
    metadata:
      name: nginx-ingress-controller-deployment
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: nginx-ingress
        template:
          metadata:
            labels:
              name: nginx-ingress
          spec:
            containers:
            - name: nginx-ingress-controller
              image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
              args:
              - /nginx-ingress-controler 
              - -- configmap=$(POD_NAMESPACE)/nginx-configuration
              env:
              - name: POD_NAME 
                valueFrom:
                  fieldRef:
                    fieldPath metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath metadata.namespace
              ports: 
              - name: http
                containerPort: 80 
              - name: https
                containerPort: 443
    ---
    apiVersion: v1
    kind: Service
    metadata: 
      name: nginx-ingress
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: 80
        protocol: TCP
        name: http 
      - port: 443
        targetPort: 443
        protocol: TCP
        name: https
      selector:
        name: nginx-ingress
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: nginx-ingress-serviceaccount
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: nginx-ingress-controler-config
    data: 
      ... 

All that is needed for a very basic Ingress Controler - thses are highlighted on provider site

You can create Ingress "template" 
  kubectl create ingress <name> --rule="host/path=service:port"
  kubectl create ingress <name> --rule="wear.my-online-store.com/wear*=wear-service:80"

Ingress example:
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: ingress-wildcard-host
    spec:
      rules:
      - host: "foo.bar.com"
        http:
          paths:
          - pathType: Prefix
            path: "/bar"
            backend:
              service:
                name: service1
                port:
                  number: 80
      - host: "*.foo.com"
        http:
          paths:
          - pathType: Prefix
            path: "/foo"
            backend:
              service:
                name: service2
                port:
                  number: 80

To list ingresses
  kubectl get ing -A

Annotations are a sort of configuration for how the request should be relayed to services
For example:
  annotations:                                                    --->>> This will tell the controler to not past the request as is with e.g. path /something
    nginx.ingress.kubernetes.io/rewrite-target: /                       this might cause error as the app under this service might not have the /something path and just serves / "root"
                                                                        hence this will translate the "/something" into "/" for the applies

###################### Planing and creating a cluster ######################
