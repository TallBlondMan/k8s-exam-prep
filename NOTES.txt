kubectl get all -l foo=bar - to get all of items with labels
kubectl replace --force -f sample.yaml - replace the created item with a new one(delete+apply/create)

1. ##################### To taint a Node: #####################
    kubectl taint nodes <node-name> key=value:(taint-effect) 
    taint-effect:
        1. NoSchedule
        2. PreferNoSchedule
        3. NoExecute
2. ##################### To make pod tolerant: #####################
    spec:
    tolerations:
    - key: "app"
        operator: "Equal"
        value: "blue"
        effect: NoSchedule

3. ##################### Node Labels and Affinity: #####################
    kubectl label nodes <node-name> <label-key>=<label-value> - put a label on a node
    To make pod select a node:
    spec:
    nodeSelector:
        label-key: label-value
4. ##################### Pod Affinity: #####################
    spec:
    affinity:
        nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:    --->>> will NOT deploy if no label found
                            OR
        preferredDuringSchedulingIgnoredDuringExecution:   --->>> will deploy if no label found
            nodeSelectorTerms:
            - matchExpressions:
            - key: label-key
                operator: In --->>> https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators 
                values:
                - label-value1
                - label-value2
                - ...

5. ##################### Pod RESOURCE and LIMIT: #####################
    resources:
      requests:     --->>> How much will it want on start
        memory: 1Gi
        cpu: 1
      limits:       --->>> How much will it allow to get
        memory: 2Gi -->> If memory is exeded it will terminate the pod with OOM status
        cpu: 3      -->> CPU will be throteled
Requests SET and NO limits are the most optimal way
We can define a LimitRange - it can be set for NAMESPACEs:
LimitRange only apply to newly created pods!!
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: cpu-resource-constraint
    spec:
      limits:
      - default:
          cpu: 500m
        defaultRequest:
          cpu: 500m
        max:
          cpu: 1
        min:
          cpu: 100m
        type: Container  
Another way is to set ResourceQuota - it's a Namespace object:
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: some-rq 
    spec:
      hard:
        requset.cpu: 4
        request.memory: 4Gi
        limits.cpu: 10
        limits.memory: 10Gi
PODs cannot be fully edited, most of fields are blocked (image, dedlineSeconds and tolerations can)
    kubectl get pod <pod-name> -o yaml > the-pod-to-edit.yaml --->>> and then kubectl delete <pod-name> --->>> kubectl create -f the-pod-to-edit.yaml

Deployment template can be change all together 
So it's better to edit a deploymet pod in a deployment file
    kubectl edit deploy <deployment-name> 

*** In describe there is LastState: <Reason> - for pod termination *** 

6. ##################### DaemonSets - simmilar to ReplicaSet, it's used for monitoring and networking #####################
                          *** Easy to modify deployed yaml for DaemonSet yaml ***
Past 1.12 it uses NodeAffinity to deploy to nodes
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: monitoring-d
    spec:
      selector:
        matchLabels:
          app: monitoring
      template:
        metadata:
          labels:
            app: monitoring
        spec:
          containers:
          - name: monitoring-agent
            image: minitoring-agent

7. ##################### Static PODs - created on Node by "kubelet" in a designated folder #####################
*** They do not have the random number in the name???!!! ***
*** LERN WHERE THE SERVICES FILES RESIDE ***      --->> By default in /etc/kubernetes/manifests 
It's defined in the kublet.service file as either:
 - --pod-manifest-path=/etc/some/path
 - --config=kubeconfig.yaml => staticPodPath=/etc/some/path


8. ##################### Multiple Schedulers: #####################
Must have different names 
Deploy additional scheduler as a POD:           --->>> To assign a POD to scheduler need to add "schedulerName: into spec: 

    **my-schedul.yaml**
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: my-scheduler               --->>> the uniqe name
    leaderElection:                             --->>> Only one leader can be selected
      leaderElect: true
      ressourceNamespace: kube-system
      ressourceName: lock-object-my-scheduler

    **custom-scheduler-as-pod.yaml**
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-custom-scheduler
      namepsace: kube-system
    spec:
      containers:
      - command:
        - kube-scheduler
        - --address=127.0.0.1
        - --kubeconfig=/path/to/scheduler.conf  --->> This drive has to be mount first into the pod
        - --config=/path/to/my-schedul.yaml     --->> This drive has to be mount first into the pod

        image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
        name: kube-scheduler

 9. ##################### Scheduler Profiles ########################
 Pods first end up in scheduling queue          --->>> this can be modified assigining priorityClass: on pod spec: | this class has to be created via "kind: PriorityClass"
Then it scores the nodes, the more resource stay the better
Then it's binding
Phases:
Scheduling Queue    ->     Filtering      ->       Scoring      ->      Binding      -->> Phases
  (queueSort)              (filter)               (score)               (bind)      -->> Extension Points - these are the extensions to which a custom plugin can bind(there are pre and post)
- PrioritySort      - NodeRessourceFit      - NodeRessourceFit     - DefaultBinder  -----------------------------
                    - NodeName              - ImageLocality                         -->> These are the pluggins
                    - NodeUnscheduable

10. ################# Monitoring ########################
For Minikube - minikube addons enable metrics-server 
Others - git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git 
** kubectl top node **  -->> to monitor nodes
** kubectl top pod **   -->> to monitor pods 

11. ##################### Application Lifecycle #####################
This applies to deployments
kubectl rollout status deployment/my-deployment         --->>> Checking the status of rollout on deployment 
kubectl rollout history deployment/my-deployment        --->>> Checking the Revisions history 

kubectl apply -f my-deployment.yaml                     --->>> Rollout a change if there are changes made to file
kubectl rollout undo deployment/my-deployment           --->>> Back out of a rollout

Deployment strategies:
  1. Recreate - All pods go down and get replaced
  2. Rolling Update(default) - Pods go down one by one and replace the same way

12. ##################### Application Configuration: #####################

  containers: 
  - name: ...
    command:                          --->>> RUN COMMAND IN A CONTAINER
    - "command"     -->> ENTRYPOINT
    args:           -->> CMD

kubectl run nginx --image=nginx --command -- <CMD> <ARG> ... <ARG>      -->> This -- sends command into container, otehr words separates commands for pod and container

POD ENVIRONMENT:
    spec:
      containers:
        env:        --->>> It's an Array
        - name: APP_VERSION
          value: "1.0.1"
        - name: APP_ENV
          value: "prod"

#####################  ConfigMaps: ######################  
These are sets of ENVIRONMENT variables for pods

** ConfigMap: **                                    --->>> To crate ConfigMap: kubectl create configmap <name> --from-literal=APP_ENV=prod 
    apiVersion: v1                                                         OR: kubectl create -f myconfig-map.yaml 
    kind: ConfigMap
    metadata:
      name: my-config-map
    data:                                           --->>> Uses data instead of spec
      APP_VERSION: 1.0.1
      APP_ENV: prod
Use configmap in Pod:
  spec:
    containers:
    - ...
      envFrom:
        - configMapRef:
          name: my-config-map
        - ...

#####################  Secrets: ##################### 

Work tha same as ConfigMaps can be created with     --->>> kubectl create secret <name> --from-literal=KEY=value
Best to use "echo -n 'value' | base64" ==> then store it in plain text
This is still bad and can be doceded - use Encription At Rest
Additionaly secrets and configmaps can be injected into POD via:

  spec:                            --->>> whole secret 
    containers:
    - ...
      envFrom:
        - secretRef:
          name: secret
        - ...
  spec:                             --->>> single secret 
    containers:
    - env:
        name: secrete_passwrd
        valueFrom:
          secretKeyRef:
            name: <name_of_secret_object>
            key: <the_key_of_desired_secret>

  containers:                       --->>> volume mount
  - name: ...
    volumeMounts:
    - name: the-secret-volume
      mountPath: "/path/to/secret"
  volumes:                          
  - name: the-secret-volume
    secret:
      secretName: db-passwrd

14. ##################### Multi Container Pod: #####################

Share the same network and can call each other localhost
Part of the same Pod object, containers: definition is an array hence:

  spec:
    containers:
    - name: the-big-db
      image: mongodb
      ...
    - name: the-small-monitor
      image: alpine
      ...

There are 3 common patterns for multicontainer POD:
-SIDECAR
-ADAPTER
-AMBASADOR

##################### Init Containers #####################
They run before the actuall containers, one by one untill completion
If one fails the pod is restarted untill success or if restartPolicy: Never it fails

You can get logs from that containers as well
** kubectl logs <pod-name> -c <init-container-name>

Might be used as a healthcheck for another service:
for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1

Or to clone a repo to a mounted volume

##################### Cluster Maintanance #####################

** NODE Draining and cordining: **
kubectl drain node01              --->>> This will shut down all pods on nodes and mark the node as unavailable for use in upgrading 
                                          Pods have to be part of Deployment/replicaSet for this to work
kubectl cordon node01             --->>> This just MARKS the node as unavailable
kubectl uncordon node01           --->>> This just UN-MARKS the node so that pods can be scheduled back on it

** Kubernetes versions **

https://github.com/kubernetes/kubernetes

##################### CLUSTER Update #####################
https://v1-27.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

Kube-apiserver is the brain X
ControllManager and kube-scheduler might be 1 version lower X-1
kubelet and kube-proxy might be 2 versions lower X-2
kubectl can be 1 ver higher or lower X-1 or X+1 

* Upgrade Options: *
1. CloudProvider - will do it for you when installed on CloudProvider
2. kubeadm - when you installed with kubeadm
3. The Hard Way - when you installed all componets by yourself

* Upgrade schema: *
ALWAYS UPGRADE ONE MINOR UP 1.10.1 -->> 1.11.0 or 1.11.0 -->> 1.12.0
1st - Upgrade Master Nodes
2nd - Upgrade the worker Nodes

* Upgrade strategy: *
Upgrade Master then all nodes at once - tha 
Upgrade Master then node by node - less app downtime
Upgrade MAster then provision new machines with new version - Cloud Provider

* How to do it with kubeadm *

  *** master node upgrade ***
  kubeadm upgrade plan
  apt-get upgrade kubeadm=1.12.0-00
  kubeadm upgrade apply v1.12.0         -->> This command will be displayd by the "plan" command
  --- kubeapi is upgraded ---

  *** kubelet upgrade ***               -->> There is no kubelet on master node when installing from scrach
  apt-get upgrade kubelet=1.12.0
  systemctl restart kubelet
  --- kubelet is upgraded ---

  *** Upgrade the nodes ***
  kubectl drain <node-name>
  ssh into node
  apt-get upgrade kubeadm=1.12.0-00
  apt-get upgrade kubelet=1.12.0
  kubeadm upgrade node config --kubelet-version v1.12 
  systemctl restart kubelet
  ssh back
  kubectl uncordon <node-name>
  --- Upgrade of nodes done --- 

##################### CLUSTER BackUp #####################

The configuration in declarative way is saved in files
But you can ask API for all imperative changes:
  kubectl get all -A -o yaml > current-config.yaml

*VELERO* - 3rd party

ETCD (export ETCDCTL_API=3 --> before use)- holds all the info of cluster so the location of it's data can be backed up
  ps -aux | grep etcd
  systemctl status etcd.service

OR
########### Create Backup ##############
Tell the etcdctl to snapshot the config:

  etcdctl snapshot save saved-config.db 

Side Note: 
 - you might need to use flags:
    --cacert
    --cert 
    --endpoints=[127.0.0.1:2379]
    --key
This above is used when etcdctl is comunicating with a POD, this is the case with kubeadm deployment
You need to specify it for the ETCDCTL to know from where to get the backup
If the ETCD is TLS encrypted

########### Restore Backup ###########

To use it for restore you have to stop kube-apiserver.service:
  systemctl stop kube-apiserver
  etcdctl snapshot restore saved-config.db --data-dir /var/lib/new-data-dir

And then configure etcd.service or POD to use new data directory:
  --data-dir=/var/lib/new-data-dir

For POD you have to fiddle with pod definition in manifest folder:
  etcdctl --data-dir=/var/lib/new-data-dir

And add the new directory as a volume path:
  volumes:
  - hostPath:
      Path: /var/lib/new-data-dir

  Reload services and start back! 

kubectl config - managing multiple clusters, changing context
etcdctl \
 --endpoints=https://127.0.0.1:2379 \
 --cacert=/etc/etcd/pki/ca.pem \
 --cert=/etc/etcd/pki/etcd.pem \
 --key=/etc/etcd/pki/etcd-key.pem \
  member list

kube-apiserver - has the etcd address           -->> POD
kubelet - might also have this configuration    -->> SERVICE

If ETCD is a remote cluster - the backups have to be made with --endpoints --cacert --cert --key 

Important if ETCD is a service and you create a --data-dir=<new-dir> you HAVE to allow only etcd user to access it:
chown -R etcd /<new-dir> 

###################### Security ######################

Communication betwen API and other control services uses TSL

## USERS ## 
Auth via kube-apiserver:
  Basic option is to create a CSV file with password,user,uid columns
  and pass it as --basic-auth-file=<the-file.csv> in kube-apiserver definition

  Then you need to create according roles for them and they can authenticate on API server:
  curl -v -k https://localhost:6443/api/v1/pods -u user:password

## Certificates ##

"The Hard Way"
You have to create all the certs by yourself - this is tedious

"kubeadm" - auto provisioning tool
This creates the certs for you and the information about them(location) is stored in the kube-apiserver.yml definition

To check if the cert is  good run 
"openssl x509 -in /path/to/certificate.crt -text -noout"
Then you need to check if the cert is Expired(date)                     --->>> you can check expiration with "kubeadm certs check-expiration"
If the Issuer is valid
Subject
Organization
And alternative names

The list of certificates is available on - https://kubernetes.io/ 

To check for issues with logs go:
  journalctl -u etcd.service -l - the hard way
  kubectl logs etcd-pod - the kubeadm way
  docker logs k8s-container-etcd - if the kubectl broke down

