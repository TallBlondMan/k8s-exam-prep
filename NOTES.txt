kubectl get all -l foo=bar - to get all of items with labels
kubectl replace --force -f sample.yaml - replace the created item with a new one(delete+apply/create)

1. ##################### To taint a Node: #####################
    kubectl taint nodes <node-name> key=value:(taint-effect) 
    taint-effect:
        1. NoSchedule
        2. PreferNoSchedule
        3. NoExecute
2. ##################### To make pod tolerant: #####################
    spec:
    tolerations:
    - key: "app"
        operator: "Equal"
        value: "blue"
        effect: NoSchedule

3. ##################### Node Labels and Affinity: #####################
    kubectl label nodes <node-name> <label-key>=<label-value> - put a label on a node
    To make pod select a node:
    spec:
    nodeSelector:
        label-key: label-value
4. ##################### Pod Affinity: #####################
    spec:
    affinity:
        nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:    --->>> will NOT deploy if no label found
                            OR
        preferredDuringSchedulingIgnoredDuringExecution:   --->>> will deploy if no label found
            nodeSelectorTerms:
            - matchExpressions:
            - key: label-key
                operator: In --->>> https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators 
                values:
                - label-value1
                - label-value2
                - ...

5. ##################### Pod RESOURCE and LIMIT: #####################
    resources:
      requests:     --->>> How much will it want on start
        memory: 1Gi
        cpu: 1
      limits:       --->>> How much will it allow to get
        memory: 2Gi -->> If memory is exeded it will terminate the pod with OOM status
        cpu: 3      -->> CPU will be throteled
Requests SET and NO limits are the most optimal way
We can define a LimitRange - it can be set for NAMESPACEs:
LimitRange only apply to newly created pods!!
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: cpu-resource-constraint
    spec:
      limits:
      - default:
          cpu: 500m
        defaultRequest:
          cpu: 500m
        max:
          cpu: 1
        min:
          cpu: 100m
        type: Container  
Another way is to set ResourceQuota - it's a Namespace object:
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: some-rq 
    spec:
      hard:
        requset.cpu: 4
        request.memory: 4Gi
        limits.cpu: 10
        limits.memory: 10Gi
PODs cannot be fully edited, most of fields are blocked (image, dedlineSeconds and tolerations can)
    kubectl get pod <pod-name> -o yaml > the-pod-to-edit.yaml --->>> and then kubectl delete <pod-name> --->>> kubectl create -f the-pod-to-edit.yaml
Deployment template can be change all together - so it's better to edit a deploymet pod in a deployment file
    kubectl edit deploy <deployment-name> 

*** In describe there is LastState: <Reason> - for pod termination *** 

6. ##################### DaemonSets - simmilar to ReplicaSet, it's used for monitoring and networking #####################
*** Easy to modify deployed yaml for DaemonSet yaml ***
Past 1.12 it uses NodeAffinity to deploy to nodes
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: monitoring-d
    spec:
      selector:
        matchLabels:
          app: monitoring
      template:
        metadata:
          labels:
            app: monitoring
        spec:
          containers:
          - name: monitoring-agent
            image: minitoring-agent

7. ##################### Static PODs - created on Node by "kubelet" in a designated folder #####################
*** They do not have the random number in the name???!!! ***
*** LERN WHERE THE SERVICES FILES RESIDE *** 
It's defined in the kublet.service file as either:
 - --pod-manifest-path-/etc/some/path
 - --config=kubeconfig.yaml => staticPodPath=/etc/some/path


8. ##################### Multiple Schedulers: #####################
Must have different names 
Deploy additional scheduler as a POD:           --->>> To assign a POD to scheduler need to add "schedulerName: into spec: 

    **my-schedul.yaml**
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: my-scheduler               --->>> the uniqe name
    leaderElection:                             --->>> Only one leader can be selected
      leaderElect: true
      ressourceNamespace: kube-system
      ressourceName: lock-object-my-scheduler

    **custom-scheduler-as-pod.yaml**
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-custom-scheduler
      namepsace: kube-system
    spec:
      containers:
      - command:
        - kube-scheduler
        - --address=127.0.0.1
        - --kubeconfig=/path/to/scheduler.conf  --->> This drive has to be mount first into the pod
        - --config=/path/to/my-schedul.yaml     --->> This drive has to be mount first into the pod

        image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
        name: kube-scheduler

 9. ##################### Scheduler Profiles ########################
 Pods first end up in scheduling queue          --->>> this can be modified assigining priorityClass: on pod spec: | this class has to be created via "kind: PriorityClass"
Then it scores the nodes, the more resource stay the better
Then it's binding
Phases:
Scheduling Queue    ->     Filtering      ->       Scoring      ->      Binding      -->> Phases
  (queueSort)              (filter)               (score)               (bind)      -->> Extension Points - these are the extensions to which a custom plugin can bind(there are pre and post)
- PrioritySort      - NodeRessourceFit      - NodeRessourceFit     - DefaultBinder  -----------------------------
                    - NodeName              - ImageLocality                         -->> These are the pluggins
                    - NodeUnscheduable

10. ################# Monitoring ########################
For Minikube - minikube addons enable metrics-server 
Others - git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git 
** kubectl top node **  -->> to monitor nodes
** kubectl top pod **   -->> to monitor pods 

11. ##################### Application Lifecycle #####################
This applies to deployments
kubectl rollout status deployment/my-deployment         --->>> Checking the status of rollout on deployment 
kubectl rollout history deployment/my-deployment        --->>> Checking the Revisions history 

kubectl apply -f my-deployment.yaml                     --->>> Rollout a change if there are changes made to file
kubectl rollout undo deployment/my-deployment           --->>> Back out of a rollout

Deployment strategies:
  1. Recreate - All pods go down and get replaced
  2. Rolling Update(default) - Pods go down one by one and replace the same way

12. ##################### Application Configuration: #####################
command:
- "command"     -->> ENTRYPOINT
args:           -->> CMD

kubectl run nginx --image=nginx --command -- <CMD> <ARG> ... <ARG>      -->> This -- sends command into container, otehr words separates commands for pod and container
 